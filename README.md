# Financial KPIs Analysis and Distress Prediction

## Project Overview

This project investigates the prediction of corporate financial distress by combining firm-level financial Key Performance Indicators (KPIs) with macroeconomic variables. Using a supervised learning framework, several statistical and machine learning models are implemented, compared, and evaluated in order to assess their predictive performance.

The objective is twofold:
1. To evaluate the predictive power of accounting-based financial ratios for distress prediction.
2. To assess whether the inclusion of macroeconomic information and dynamic features (year-over-year changes) improves model performance.


---

## Data Description

### Raw Data (`data/raw/`)

The analysis relies on two main raw datasets:

- **Compustat firm-level data (`compustat_data.csv`)**  
  Contains firm-level accounting information over time, including balance sheet and income statement variables used to construct financial KPIs.

- **Macroeconomic data (`macro_data.csv`)**  
  Yearly macroeconomic indicators such as GDP growth, inflation, interest rates, and unemployment rates.

The `compustat_data.csv` datasets needs to be uploaded before running the code as the file is too heavy.

---

### Processed Data (`data/processed/`)

Intermediate datasets created during preprocessing:

- **`compustat_kpis.csv`**  
  Cleaned firm-level dataset including the following financial KPIs:
  - Return on Assets (ROA)
  - Total Debt to Equity
  - Current Ratio
  - Cash Flow from Operations Margin
  - Asset Turnover

- **`macro_clean_with_deltas.csv`**  
  Cleaned macroeconomic dataset including both variable levels and year-over-year changes (deltas).

---

### Final Dataset (`data/final/`)

- **`panel_balanced_with_deltas.csv`**  
  Final panel dataset used for modeling. It combines:
  - Firm-level KPIs
  - KPI deltas
  - Macroeconomic variables
  - Macroeconomic deltas  

  The dataset is balanced at the firm level between financially distressed and healthy firms.

---

## Code Structure

### Entry Point

- **`main.py`**  
  This is the main entry point of the project. Running:
  ```bash
  python main.py

executes the full pipeline:
- Loads the final dataset
- Splits the data into training and testing sets
- Trains all models
- Evaluates model performance
- Generates and saves all figures used in the report

---

## Source Code (`src/`)

### `data_loader.py`
Handles data loading and preprocessing, including:
- Reading the final panel dataset
- Handling missing values
- Feature selection
- Preparing inputs for modeling

### `models.py`
Defines and trains the supervised learning models:
- Logistic Regression
- Random Forest
- XGBoost

### `evaluation.py`
Contains evaluation and visualization utilities:
- Confusion matrices
- ROC curves
- Feature importance plots

All figures generated by these utilities are automatically saved to the `results/` directory.

### `__init__.py`
Declares the `src` directory as a Python package to enable clean imports.

---

## Results (`results/`)

This folder contains all figures generated automatically by running `main.py`, including:
- Confusion matrices for each model
- ROC curve comparison across models
- Feature importance plots for Random Forest and XGBoost

Only figures directly related to model evaluation and included in the report are stored here.

---

## Notebooks (`notebooks/`)

Jupyter notebooks are provided for exploratory data analysis (EDA), including:
- Missing value analysis of financial KPIs
- Correlation matrix between KPIs
- Preliminary descriptive statistics

These analyses are informative and descriptive but **not part of the automated pipeline**, as they do not directly address the predictive task.

---

## Models Implemented

Three supervised learning models are evaluated:

1. **Logistic Regression**  
   Serves as a baseline model, offering interpretability and a theoretical benchmark.

2. **Random Forest**  
   A non-linear ensemble method capable of capturing complex interactions between firm-level and macroeconomic variables.

3. **XGBoost**  
   A gradient boosting model that achieves the highest predictive performance across most evaluation metrics.

---

## Reproducibility

Reproducibility is ensured by:
- Fixing `random_state` parameters for all train-test splits and models
- Using deterministic preprocessing steps
- Explicitly listing all dependencies

Running the pipeline multiple times produces identical results.

---

## Dependencies

The project requires **Python 3.10+**.

All dependencies are listed in `requirements.txt` and can be installed with:

```bash
pip install -r requirements.txt
